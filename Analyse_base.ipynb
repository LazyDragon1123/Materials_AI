{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import functools as f\n",
    "from pymatgen import MPRester\n",
    "from copy import deepcopy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pymatgen as mg\n",
    "import os\n",
    "import itertools \n",
    "\n",
    "API_KEY = \"oq6bIVf0EQMI6dun\"\n",
    "mp = MPRester(API_KEY)\n",
    "save_path = '/Users/dragonlook/Dropbox (MIT)/MI/codes/MP_data/'\n",
    "\n",
    "mag_list = pd.read_excel('/Users/dragonlook/Dropbox (MIT)/MI/codes/list/thermal_magnon_list.xlsx')\n",
    "samples = list(mag_list.Formula.dropna())\n",
    "mag_elements = list(mag_list.mag_element.dropna())\n",
    "targets = list(mag_list.target.dropna())\n",
    "basic_properties = [\"energy\", \"energy_per_atom\", \"volume\",\n",
    "                                \"formation_energy_per_atom\", \"nsites\",\n",
    "                                \"unit_cell_formula\", \"pretty_formula\",\n",
    "                                \"is_hubbard\", \"elements\", \"nelements\",\n",
    "                                \"e_above_hull\", \"hubbards\", \"is_compatible\",\n",
    "                                \"spacegroup\", \"task_ids\", \"band_gap\", \"density\",\n",
    "                                \"total_magnetization\",\n",
    "                                \"material_id\", \"oxide_type\", \"tags\", \"elasticity\"]\n",
    "\n",
    "crystal_fts = ['angle', 'length','nn_sites']\n",
    "\n",
    "fts = ['energy_per_atom',\n",
    "       'formation_energy_per_atom', \n",
    "       'nsites', \n",
    "       'is_hubbard', 'elements', 'nelements', 'e_above_hull',\n",
    "       'spacegroup', 'band_gap',\n",
    "       'density', 'total_magnetization', 'oxide_type', \n",
    "       'elasticity', 'angle', 'length', 'nn_sites']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Al2O3\n",
      "Si\n",
      "CaF2\n",
      "C\n",
      "NiO\n",
      "MnO\n",
      "Ni2O3\n",
      "CoO\n",
      "CoF2\n",
      "MnF2\n",
      "ZnF2\n",
      "UO2\n",
      "K2CuF4\n",
      "CsNiF3\n",
      "RbMnF3\n",
      "KCuF3\n",
      "FeCl2\n",
      "CdCl2\n",
      "CdCl3\n",
      "CdCl3 : no data \n",
      "Y3Al5O12\n",
      "CdTe\n",
      "Mn2SiS4\n",
      "Mn2SiSe4\n",
      "NiTa2O6\n",
      "NiSb2O6\n",
      "CoTa2O6\n",
      "CoSb2O6\n",
      "CuSb2O6\n",
      "ZnSb2O6\n",
      "Co3V2O8\n",
      "NaV2O5\n",
      "LiCu2O2\n",
      "FeMn2O4 \n",
      "CuFeO2 \n",
      "MnFe2O4\n",
      "CoFe2O4\n",
      "ZnCr2Se4\n",
      "CaIrO3\n",
      "ZnCr2O4 \n",
      "CuCr2O4\n",
      "Cs2CuCl4 \n",
      "SrCu2(BO3)2 \n",
      "ZnSO4 \n",
      "Cu3BiO2(SeO3)2Cl \n",
      "Cu2OSeO3 \n",
      "Cu3Mo2O9 \n",
      "CoBr6H12O6 \n",
      "CoBr6H12O6 : no data \n",
      "MnCl2H8O4 \n",
      "Cu3(CO3)2(OH)2 \n",
      "CuGeO3\n",
      "TlCuCl3\n",
      "K2V3O8 \n",
      "Cu3B2O6 \n",
      "MnCO3 \n",
      "CoCO3 \n",
      "CsMnCl3.2H2O \n",
      "CsMnCl3.2H2O : no data \n",
      "GdVO4\n",
      "ErVO4 \n",
      "CoCl2.6H2O \n",
      "CoCl2.6H2O : no data \n",
      "NiCl2S4C4N8H16 \n",
      "BiCu2PO6 \n",
      "Cu2Te2O5Br2 \n",
      "(CH3)4NMnCl3 \n",
      "(CH3)4NMnCl3 : no data \n",
      "(CH3)2NH2MnCl3 \n",
      "CuCl2(CH3NH3Cl)2 \n",
      "CuCl2(CH3NH3Cl)2 : no data \n",
      "CoCs3Cl5 \n",
      "Cu(NH4)2Br42H2O \n",
      "Cu(NH4)2Br42H2O : no data \n",
      "(CH3NH3)2CuCl4 \n",
      "(CH3NH3)2CuCl4 : no data \n",
      "(C2H5NH3)2CuCl4 \n",
      "(C2H5NH3)2CuCl4 : no data \n",
      "BaCo2(AsO4)2 \n",
      "Cs4CuSb2Cl12 \n",
      "Cs4CuSb2Cl12 : no data \n",
      "CuB2O4 \n",
      "Ba3CoSb2O9 \n",
      "Y2BaNiO5 \n",
      "CsCuCl \n",
      "CsCuCl : no data \n",
      "RbCoCl3\n",
      "RbCoBr3\n",
      "CsCoCl3\n",
      "CsCoBr3\n",
      "CsCoBr3 : no data \n",
      "Tb2Ti2O7 \n",
      "TiOCl\n",
      "TiOBr \n",
      "TmVO4\n",
      "TmAsO5\n",
      "TmAsO5 : no data \n",
      "GdFeO3\n",
      "DyFeO3 \n",
      "YMnO3 \n",
      "HoMnO4\n",
      "HoMnO4 : no data \n",
      "Ba3Mn2O8 \n",
      "BaCo2V2O8 \n",
      "Ni3V2O8 \n",
      "Ba2Cu3O4Cl2 \n",
      "Sr14Cu24O41 \n",
      "Sr14Cu24O41 : no data \n",
      "SrCuO2 \n",
      "Sr2CuO3 \n",
      "AgVP2S6 \n",
      "Ca2Y2Cu5O10 \n",
      "Ca2Y2Cu5O10 : no data \n",
      "La2SrCuO4 \n",
      "La2SrCuO4 : no data \n",
      "Gd2CuO4 \n",
      "Pr2CuO4\n",
      "Nd2CuO4\n",
      "Sm2CuO4\n",
      "Eu2CuO4\n",
      "Sr2CuO2Cl2 \n",
      "RuCl3\n",
      "Cr2Ge2Te6 \n",
      "Cr2Si2Te6\n",
      "NiPS3\n",
      "MnPS3\n",
      "BaCu2Si2O7\n",
      "Sr2V3P9\n",
      "Sr2V3P9 : no data \n",
      "LiCuVO4\n",
      "La2Cu2O5\n",
      "La8Cu7O19\n",
      "La2CuO4\n",
      "Cu3Ba2O6\n",
      "La2NiO4\n",
      "La2V2O7\n",
      "La2V2O7 : no data \n",
      "Tb3Ga5O12\n",
      "SrTiO3\n",
      "KTaO3\n"
     ]
    }
   ],
   "source": [
    "# data load\n",
    "df_dict = {}\n",
    "df_targets = []\n",
    "cout = -1\n",
    "for sample in samples:\n",
    "    cout += 1\n",
    "    print(sample)\n",
    "    sample = sample.strip()\n",
    "    df_comps = pd.read_csv(save_path + sample + '.csv')\n",
    "    if len(df_comps) == 0:\n",
    "        print(sample + ' : no data ')\n",
    "        continue\n",
    "    if len(df_comps) == 1:\n",
    "        df_comp = df_comps\n",
    "        df_ind = 0\n",
    "    elif len(df_comps) > 1:\n",
    "        # take one with lowewest energy per atom\n",
    "        df_ind = list(df_comps.loc[:,'energy_per_atom']).index(min(list(df_comps.loc[:,'energy_per_atom'])))\n",
    "        df_comp = df_comps.iloc[df_ind]\n",
    "    # crystal infomation\n",
    "    crystal_fts_append = []\n",
    "    for crystal_ft in crystal_fts:\n",
    "        ft_file_name = crystal_ft\n",
    "        if crystal_ft == 'nn_sites':\n",
    "            ft_file_name = 'length'\n",
    "        file_crystal = save_path + sample + '/' + sample + '_' + ft_file_name + '_' + str(df_ind) + '.csv'\n",
    "        df_crystal = pd.read_csv(file_crystal)\n",
    "        if crystal_ft == 'angle':\n",
    "            df_crystal = np.sort(list(set(df_crystal.loc[:,'angle'])))\n",
    "            if df_crystal[0] == 0:\n",
    "                df_append = df_crystal[1]\n",
    "            else:\n",
    "                df_append = df_crystal[0]\n",
    "\n",
    "        elif crystal_ft == 'length' or 'nn_sites':\n",
    "            mean_length = []\n",
    "            mean_neighboring = []\n",
    "            for i in range(len(df_crystal)):\n",
    "                res = df_crystal.loc[0,'length'].strip('][').split(' ') \n",
    "                res = [float(i) for i in res if len(i)]\n",
    "                mean_length.append(np.mean(res))\n",
    "                mean_neighboring.append(len(res))\n",
    "\n",
    "            if crystal_ft == 'length':\n",
    "                df_append = np.mean(mean_length)\n",
    "            elif crystal_ft == 'nn_sites':\n",
    "                df_append = np.mean(mean_neighboring)\n",
    "\n",
    "        crystal_fts_append.append(df_append)\n",
    "\n",
    "    df_append = pd.DataFrame([crystal_fts_append], columns = crystal_fts)\n",
    "    if len(df_comp.index) == 1:\n",
    "        df_all = pd.concat([df_comp.T.copy(), df_append.T], axis=0)\n",
    "    else:\n",
    "        df_all = pd.concat([df_comp.copy(), df_append.T], axis=0)\n",
    "    df_all = df_all.T[fts]\n",
    "    df_dict[sample] = df_all\n",
    "    df_targets.append(targets[cout])\n",
    "\n",
    "    \n",
    "df1 = pd.concat(df_dict, axis=0)\n",
    "df1 = df1.rename_axis(['name','none'], axis=0)\n",
    "comp_lists = list(dict.fromkeys(list(df1.index.get_level_values('name'))))\n",
    "df1.to_csv(save_path + 'pre_predataprocessing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dragonlook/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:576: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n",
      "/Users/dragonlook/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "#### preprocessing ####\n",
    "#target\n",
    "def target_maker(x):\n",
    "    if x == 'm' or x == 'me':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "df_targets = [target_maker(i) for i in df_targets]\n",
    "\n",
    "lgbm_fts = ['energy_per_atom',\n",
    "       'formation_energy_per_atom', \n",
    "       'nsites', \n",
    "       'is_hubbard',  'nelements', 'e_above_hull',\n",
    "       'band_gap',\n",
    "       'density', 'total_magnetization', 'oxide_type', \n",
    "        'angle', 'length', 'nn_sites']\n",
    "bin_labs = ['is_hubbard']\n",
    "\n",
    "df2 = df1[lgbm_fts] \n",
    "\n",
    "# False and True lab\n",
    "for bin_lab in bin_labs:\n",
    "    df2.loc[:,bin_lab] = df2.copy().apply(lambda x: 1 if x[bin_lab] else 0, axis=1)\n",
    "# oxide_type\n",
    "df2.loc[:,'oxide_type'] = df2.copy().apply(lambda x: 1 if x['oxide_type']=='oxide' else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "global X_train, X_test, y_train, y_test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df2, df_targets, test_size=0.2, shuffle = True, random_state=100,\n",
    "                                                    stratify=df_targets)\n",
    "X_train = X_train.astype(float)\n",
    "# X_train = X_train.to_numpy()\n",
    "X_test = X_test.astype(float)\n",
    "# X_test = X_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "import pandas as pd\n",
    "\n",
    "import lightgbm as lgbm\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def feval_acc_lgbm(pred_proba, dtrain):\n",
    "    # 真のデータ\n",
    "    y_true = dtrain.get_label().astype(int)\n",
    "    # 予測\n",
    "    y_pred = np.where(pred_proba > 0.5, 1, 0)\n",
    "    # Accuracy を計算する\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    # メトリックの名前と数値を返す(最小化を目指すので 1 から引く)\n",
    "    return 'accuracy', 1 - acc, False\n",
    "\n",
    "def lgb_eval(num_leaves,min_data_in_leaf,lambda_l2):\n",
    "\n",
    "        train_df_x = pd.DataFrame(X_train)\n",
    "        train_df_y = pd.DataFrame(y_train)\n",
    "        test_df_x = pd.DataFrame(X_test)\n",
    "        test_df_y = pd.DataFrame(y_test)\n",
    "\n",
    "        train_data = lgbm.Dataset(X_train, label=y_train)\n",
    "        eval_data = lgbm.Dataset(X_test, label=y_test, reference= train_data)\n",
    "\n",
    "\n",
    "        params = {\n",
    "                'boosting_type':'gbdt',\n",
    "                'eval_metric':'logloss',\n",
    "                'objective':'binary',\n",
    "                'learning_rate':0.01,              \n",
    "                'num_leaves':int(num_leaves),\n",
    "                'min_data_in_leaf':int(min_data_in_leaf),\n",
    "                'max_depth':31,\n",
    "                'lambda_l2':lambda_l2,\n",
    "                'bagging_fraction': 1.0,\n",
    "                'bagging_freq': 0,\n",
    "                }\n",
    "\n",
    "\n",
    "        cls = lgbm.train(params,\n",
    "                train_data,\n",
    "                valid_sets=eval_data,\n",
    "                num_boost_round=1000,\n",
    "                early_stopping_rounds=20,\n",
    "                verbose_eval=100,\n",
    "                )\n",
    "\n",
    "        pred = cls.predict(X_test)\n",
    "        score = 1 - log_loss(y_test, pred)\n",
    "        return score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "# from evaluator.super_accuracy import super_accuracy\n",
    "# from model.tree_visual.mapping_score import mapping_score\n",
    "\n",
    "# from model.tree.hp.lgb_eval import lgb_eval\n",
    "\n",
    "import pandas as pd\n",
    "import lightgbm as lgbm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def lightgbm_bo(train_df_x,train_df_y,test_df_x,test_df_y,num_l=200,min_d=20,l2=10):\n",
    "\n",
    "        lgb_evaluate = lgb_eval\n",
    "\n",
    "        lgb_bo = BayesianOptimization(lgb_evaluate, \n",
    "                              {'num_leaves':(20,num_l),\n",
    "                              'min_data_in_leaf':(5,min_d),\n",
    "                              'lambda_l2':(0.1,l2),                           \n",
    "                              },\n",
    "                              verbose=0\n",
    "                              )\n",
    "\n",
    "        lgb_bo.maximize(init_points=20, n_iter=50, acq='ei')\n",
    "\n",
    "        print('Optimized params are:')\n",
    "        print(lgb_bo.max['params'])\n",
    "\n",
    "        params = lgb_bo.max['params']\n",
    "        params['num_leaves'] = int(params['num_leaves'])\n",
    "        params['min_data_in_leaf'] = int(params['min_data_in_leaf'])\n",
    "\n",
    "        d1 = {\n",
    "                'boosting_type':'gbdt',\n",
    "                'eval_metric':'logloss',\n",
    "                'objective':'binary',\n",
    "                'learning_rate':0.01,\n",
    "                'max_depth':31,\n",
    "                'bagging_fraction': 1.0,\n",
    "                'bagging_freq': 0,\n",
    "        }\n",
    "\n",
    "        params.update(d1)\n",
    "\n",
    "        return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.66158\n",
      "[200]\tvalid_0's binary_logloss: 0.654317\n",
      "[300]\tvalid_0's binary_logloss: 0.647388\n",
      "[400]\tvalid_0's binary_logloss: 0.636716\n",
      "[500]\tvalid_0's binary_logloss: 0.629103\n",
      "[600]\tvalid_0's binary_logloss: 0.622779\n",
      "[700]\tvalid_0's binary_logloss: 0.618128\n",
      "[800]\tvalid_0's binary_logloss: 0.615384\n",
      "Early stopping, best iteration is:\n",
      "[877]\tvalid_0's binary_logloss: 0.613816\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[78]\tvalid_0's binary_logloss: 0.636488\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.676413\n",
      "[200]\tvalid_0's binary_logloss: 0.671454\n",
      "[300]\tvalid_0's binary_logloss: 0.668975\n",
      "[400]\tvalid_0's binary_logloss: 0.659918\n",
      "[500]\tvalid_0's binary_logloss: 0.652835\n",
      "Early stopping, best iteration is:\n",
      "[547]\tvalid_0's binary_logloss: 0.650146\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.654268\n",
      "[200]\tvalid_0's binary_logloss: 0.642508\n",
      "Early stopping, best iteration is:\n",
      "[248]\tvalid_0's binary_logloss: 0.634702\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.661227\n",
      "[200]\tvalid_0's binary_logloss: 0.653886\n",
      "[300]\tvalid_0's binary_logloss: 0.648105\n",
      "[400]\tvalid_0's binary_logloss: 0.6344\n",
      "[500]\tvalid_0's binary_logloss: 0.624887\n",
      "[600]\tvalid_0's binary_logloss: 0.616608\n",
      "[700]\tvalid_0's binary_logloss: 0.61163\n",
      "[800]\tvalid_0's binary_logloss: 0.609385\n",
      "[900]\tvalid_0's binary_logloss: 0.607795\n",
      "Early stopping, best iteration is:\n",
      "[909]\tvalid_0's binary_logloss: 0.607603\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.661914\n",
      "[200]\tvalid_0's binary_logloss: 0.655995\n",
      "[300]\tvalid_0's binary_logloss: 0.648631\n",
      "[400]\tvalid_0's binary_logloss: 0.639667\n",
      "[500]\tvalid_0's binary_logloss: 0.631018\n",
      "[600]\tvalid_0's binary_logloss: 0.625135\n",
      "[700]\tvalid_0's binary_logloss: 0.619765\n",
      "[800]\tvalid_0's binary_logloss: 0.615905\n",
      "[900]\tvalid_0's binary_logloss: 0.614062\n",
      "Early stopping, best iteration is:\n",
      "[952]\tvalid_0's binary_logloss: 0.613177\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.656988\n",
      "[200]\tvalid_0's binary_logloss: 0.62637\n",
      "[300]\tvalid_0's binary_logloss: 0.607783\n",
      "Early stopping, best iteration is:\n",
      "[369]\tvalid_0's binary_logloss: 0.603287\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.650196\n",
      "Early stopping, best iteration is:\n",
      "[100]\tvalid_0's binary_logloss: 0.650196\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.676787\n",
      "[200]\tvalid_0's binary_logloss: 0.671833\n",
      "[300]\tvalid_0's binary_logloss: 0.670069\n",
      "[400]\tvalid_0's binary_logloss: 0.663155\n",
      "[500]\tvalid_0's binary_logloss: 0.656075\n",
      "[600]\tvalid_0's binary_logloss: 0.650372\n",
      "Early stopping, best iteration is:\n",
      "[622]\tvalid_0's binary_logloss: 0.649111\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.677544\n",
      "Early stopping, best iteration is:\n",
      "[110]\tvalid_0's binary_logloss: 0.677053\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.675904\n",
      "[200]\tvalid_0's binary_logloss: 0.670653\n",
      "[300]\tvalid_0's binary_logloss: 0.665158\n",
      "[400]\tvalid_0's binary_logloss: 0.656551\n",
      "Early stopping, best iteration is:\n",
      "[466]\tvalid_0's binary_logloss: 0.65168\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.657334\n",
      "Early stopping, best iteration is:\n",
      "[157]\tvalid_0's binary_logloss: 0.655547\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.677475\n",
      "Early stopping, best iteration is:\n",
      "[86]\tvalid_0's binary_logloss: 0.677231\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[75]\tvalid_0's binary_logloss: 0.672281\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[70]\tvalid_0's binary_logloss: 0.668681\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.661299\n",
      "[200]\tvalid_0's binary_logloss: 0.651067\n",
      "[300]\tvalid_0's binary_logloss: 0.637094\n",
      "[400]\tvalid_0's binary_logloss: 0.622213\n",
      "[500]\tvalid_0's binary_logloss: 0.613036\n",
      "[600]\tvalid_0's binary_logloss: 0.608934\n",
      "Early stopping, best iteration is:\n",
      "[608]\tvalid_0's binary_logloss: 0.608211\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.676048\n",
      "[200]\tvalid_0's binary_logloss: 0.671759\n",
      "[300]\tvalid_0's binary_logloss: 0.667172\n",
      "[400]\tvalid_0's binary_logloss: 0.658142\n",
      "[500]\tvalid_0's binary_logloss: 0.651766\n",
      "Early stopping, best iteration is:\n",
      "[507]\tvalid_0's binary_logloss: 0.651208\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.667181\n",
      "[200]\tvalid_0's binary_logloss: 0.657727\n",
      "[300]\tvalid_0's binary_logloss: 0.644258\n",
      "Early stopping, best iteration is:\n",
      "[319]\tvalid_0's binary_logloss: 0.64267\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.677236\n",
      "[200]\tvalid_0's binary_logloss: 0.672665\n",
      "[300]\tvalid_0's binary_logloss: 0.670017\n",
      "[400]\tvalid_0's binary_logloss: 0.665219\n",
      "[500]\tvalid_0's binary_logloss: 0.658634\n",
      "[600]\tvalid_0's binary_logloss: 0.652679\n",
      "[700]\tvalid_0's binary_logloss: 0.648735\n",
      "Early stopping, best iteration is:\n",
      "[685]\tvalid_0's binary_logloss: 0.648432\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.65941\n",
      "[200]\tvalid_0's binary_logloss: 0.641943\n",
      "Early stopping, best iteration is:\n",
      "[196]\tvalid_0's binary_logloss: 0.641029\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.651187\n",
      "[200]\tvalid_0's binary_logloss: 0.642338\n",
      "[300]\tvalid_0's binary_logloss: 0.634988\n",
      "Early stopping, best iteration is:\n",
      "[354]\tvalid_0's binary_logloss: 0.63\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.661893\n",
      "[200]\tvalid_0's binary_logloss: 0.655888\n",
      "[300]\tvalid_0's binary_logloss: 0.648619\n",
      "[400]\tvalid_0's binary_logloss: 0.63965\n",
      "[500]\tvalid_0's binary_logloss: 0.630987\n",
      "[600]\tvalid_0's binary_logloss: 0.625104\n",
      "[700]\tvalid_0's binary_logloss: 0.619712\n",
      "[800]\tvalid_0's binary_logloss: 0.615833\n",
      "[900]\tvalid_0's binary_logloss: 0.614056\n",
      "Early stopping, best iteration is:\n",
      "[959]\tvalid_0's binary_logloss: 0.613334\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.661225\n",
      "Early stopping, best iteration is:\n",
      "[168]\tvalid_0's binary_logloss: 0.648915\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.65291\n",
      "[200]\tvalid_0's binary_logloss: 0.64112\n",
      "[300]\tvalid_0's binary_logloss: 0.634505\n",
      "Early stopping, best iteration is:\n",
      "[283]\tvalid_0's binary_logloss: 0.632884\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.65358\n",
      "[200]\tvalid_0's binary_logloss: 0.64138\n",
      "Early stopping, best iteration is:\n",
      "[276]\tvalid_0's binary_logloss: 0.632023\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.655197\n",
      "[200]\tvalid_0's binary_logloss: 0.644373\n",
      "[300]\tvalid_0's binary_logloss: 0.628682\n",
      "Early stopping, best iteration is:\n",
      "[313]\tvalid_0's binary_logloss: 0.627397\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[68]\tvalid_0's binary_logloss: 0.628814\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.662007\n",
      "[200]\tvalid_0's binary_logloss: 0.657744\n",
      "[300]\tvalid_0's binary_logloss: 0.650682\n",
      "[400]\tvalid_0's binary_logloss: 0.643184\n",
      "[500]\tvalid_0's binary_logloss: 0.634301\n",
      "[600]\tvalid_0's binary_logloss: 0.628311\n",
      "[700]\tvalid_0's binary_logloss: 0.622744\n",
      "[800]\tvalid_0's binary_logloss: 0.618554\n",
      "[900]\tvalid_0's binary_logloss: 0.614457\n",
      "[1000]\tvalid_0's binary_logloss: 0.613162\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[992]\tvalid_0's binary_logloss: 0.613081\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.662131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's binary_logloss: 0.657452\n",
      "[300]\tvalid_0's binary_logloss: 0.650298\n",
      "[400]\tvalid_0's binary_logloss: 0.642974\n",
      "[500]\tvalid_0's binary_logloss: 0.633934\n",
      "[600]\tvalid_0's binary_logloss: 0.627774\n",
      "[700]\tvalid_0's binary_logloss: 0.622504\n",
      "[800]\tvalid_0's binary_logloss: 0.618191\n",
      "[900]\tvalid_0's binary_logloss: 0.614689\n",
      "[1000]\tvalid_0's binary_logloss: 0.61304\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[999]\tvalid_0's binary_logloss: 0.612905\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.660558\n",
      "[200]\tvalid_0's binary_logloss: 0.652627\n",
      "[300]\tvalid_0's binary_logloss: 0.646642\n",
      "[400]\tvalid_0's binary_logloss: 0.640306\n",
      "[500]\tvalid_0's binary_logloss: 0.637528\n",
      "Early stopping, best iteration is:\n",
      "[554]\tvalid_0's binary_logloss: 0.636481\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.651187\n",
      "[200]\tvalid_0's binary_logloss: 0.642338\n",
      "[300]\tvalid_0's binary_logloss: 0.634988\n",
      "Early stopping, best iteration is:\n",
      "[354]\tvalid_0's binary_logloss: 0.63\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.651187\n",
      "[200]\tvalid_0's binary_logloss: 0.642338\n",
      "[300]\tvalid_0's binary_logloss: 0.634988\n",
      "Early stopping, best iteration is:\n",
      "[354]\tvalid_0's binary_logloss: 0.63\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.661922\n",
      "[200]\tvalid_0's binary_logloss: 0.657525\n",
      "[300]\tvalid_0's binary_logloss: 0.649918\n",
      "[400]\tvalid_0's binary_logloss: 0.641958\n",
      "[500]\tvalid_0's binary_logloss: 0.632991\n",
      "[600]\tvalid_0's binary_logloss: 0.626951\n",
      "[700]\tvalid_0's binary_logloss: 0.621602\n",
      "[800]\tvalid_0's binary_logloss: 0.617273\n",
      "[900]\tvalid_0's binary_logloss: 0.614214\n",
      "[1000]\tvalid_0's binary_logloss: 0.613148\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[983]\tvalid_0's binary_logloss: 0.612914\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.612969\n",
      "Early stopping, best iteration is:\n",
      "[103]\tvalid_0's binary_logloss: 0.612749\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.659297\n",
      "[200]\tvalid_0's binary_logloss: 0.646624\n",
      "[300]\tvalid_0's binary_logloss: 0.634481\n",
      "[400]\tvalid_0's binary_logloss: 0.624165\n",
      "Early stopping, best iteration is:\n",
      "[420]\tvalid_0's binary_logloss: 0.622384\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[57]\tvalid_0's binary_logloss: 0.669752\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[77]\tvalid_0's binary_logloss: 0.622997\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.662077\n",
      "[200]\tvalid_0's binary_logloss: 0.657462\n",
      "[300]\tvalid_0's binary_logloss: 0.649924\n",
      "[400]\tvalid_0's binary_logloss: 0.64178\n",
      "[500]\tvalid_0's binary_logloss: 0.632601\n",
      "[600]\tvalid_0's binary_logloss: 0.626498\n",
      "[700]\tvalid_0's binary_logloss: 0.621455\n",
      "[800]\tvalid_0's binary_logloss: 0.61694\n",
      "[900]\tvalid_0's binary_logloss: 0.614121\n",
      "Early stopping, best iteration is:\n",
      "[975]\tvalid_0's binary_logloss: 0.612993\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.628933\n",
      "Early stopping, best iteration is:\n",
      "[81]\tvalid_0's binary_logloss: 0.62576\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.615034\n",
      "Early stopping, best iteration is:\n",
      "[83]\tvalid_0's binary_logloss: 0.614218\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[59]\tvalid_0's binary_logloss: 0.668832\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.650396\n",
      "[200]\tvalid_0's binary_logloss: 0.646973\n",
      "Early stopping, best iteration is:\n",
      "[279]\tvalid_0's binary_logloss: 0.644719\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.640357\n",
      "Early stopping, best iteration is:\n",
      "[122]\tvalid_0's binary_logloss: 0.637907\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.65066\n",
      "[200]\tvalid_0's binary_logloss: 0.647342\n",
      "[300]\tvalid_0's binary_logloss: 0.645174\n",
      "Early stopping, best iteration is:\n",
      "[292]\tvalid_0's binary_logloss: 0.645028\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[68]\tvalid_0's binary_logloss: 0.628814\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.65095\n",
      "[200]\tvalid_0's binary_logloss: 0.647807\n",
      "[300]\tvalid_0's binary_logloss: 0.645203\n",
      "Early stopping, best iteration is:\n",
      "[291]\tvalid_0's binary_logloss: 0.645027\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.634011\n",
      "Early stopping, best iteration is:\n",
      "[80]\tvalid_0's binary_logloss: 0.62275\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.653824\n",
      "[200]\tvalid_0's binary_logloss: 0.641737\n",
      "[300]\tvalid_0's binary_logloss: 0.631437\n",
      "Early stopping, best iteration is:\n",
      "[303]\tvalid_0's binary_logloss: 0.63033\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.6621\n",
      "[200]\tvalid_0's binary_logloss: 0.657885\n",
      "[300]\tvalid_0's binary_logloss: 0.650736\n",
      "[400]\tvalid_0's binary_logloss: 0.643275\n",
      "[500]\tvalid_0's binary_logloss: 0.634599\n",
      "[600]\tvalid_0's binary_logloss: 0.62818\n",
      "[700]\tvalid_0's binary_logloss: 0.622878\n",
      "[800]\tvalid_0's binary_logloss: 0.61871\n",
      "[900]\tvalid_0's binary_logloss: 0.614752\n",
      "[1000]\tvalid_0's binary_logloss: 0.612937\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's binary_logloss: 0.612937\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.65095\n",
      "[200]\tvalid_0's binary_logloss: 0.647807\n",
      "[300]\tvalid_0's binary_logloss: 0.645203\n",
      "Early stopping, best iteration is:\n",
      "[291]\tvalid_0's binary_logloss: 0.645027\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.661046\n",
      "[200]\tvalid_0's binary_logloss: 0.650372\n",
      "[300]\tvalid_0's binary_logloss: 0.64101\n",
      "[400]\tvalid_0's binary_logloss: 0.631052\n",
      "[500]\tvalid_0's binary_logloss: 0.623028\n",
      "[600]\tvalid_0's binary_logloss: 0.618871\n",
      "[700]\tvalid_0's binary_logloss: 0.616507\n",
      "[800]\tvalid_0's binary_logloss: 0.614451\n",
      "Early stopping, best iteration is:\n",
      "[788]\tvalid_0's binary_logloss: 0.614319\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.683537\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.651187\n",
      "[200]\tvalid_0's binary_logloss: 0.642338\n",
      "[300]\tvalid_0's binary_logloss: 0.634988\n",
      "Early stopping, best iteration is:\n",
      "[354]\tvalid_0's binary_logloss: 0.63\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.651187\n",
      "[200]\tvalid_0's binary_logloss: 0.642338\n",
      "[300]\tvalid_0's binary_logloss: 0.634988\n",
      "Early stopping, best iteration is:\n",
      "[354]\tvalid_0's binary_logloss: 0.63\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.65095\n",
      "[200]\tvalid_0's binary_logloss: 0.647807\n",
      "[300]\tvalid_0's binary_logloss: 0.645203\n",
      "Early stopping, best iteration is:\n",
      "[291]\tvalid_0's binary_logloss: 0.645027\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[33]\tvalid_0's binary_logloss: 0.670259\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.650311\n",
      "[200]\tvalid_0's binary_logloss: 0.646761\n",
      "Early stopping, best iteration is:\n",
      "[272]\tvalid_0's binary_logloss: 0.644536\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.653529\n",
      "[200]\tvalid_0's binary_logloss: 0.641214\n",
      "[300]\tvalid_0's binary_logloss: 0.63278\n",
      "Early stopping, best iteration is:\n",
      "[286]\tvalid_0's binary_logloss: 0.631737\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.650612\n",
      "Early stopping, best iteration is:\n",
      "[157]\tvalid_0's binary_logloss: 0.648061\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.683572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.657084\n",
      "[200]\tvalid_0's binary_logloss: 0.645985\n",
      "[300]\tvalid_0's binary_logloss: 0.638844\n",
      "[400]\tvalid_0's binary_logloss: 0.635217\n",
      "Early stopping, best iteration is:\n",
      "[412]\tvalid_0's binary_logloss: 0.634409\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.662201\n",
      "[200]\tvalid_0's binary_logloss: 0.657712\n",
      "[300]\tvalid_0's binary_logloss: 0.650542\n",
      "[400]\tvalid_0's binary_logloss: 0.643049\n",
      "[500]\tvalid_0's binary_logloss: 0.633874\n",
      "[600]\tvalid_0's binary_logloss: 0.627779\n",
      "[700]\tvalid_0's binary_logloss: 0.622609\n",
      "[800]\tvalid_0's binary_logloss: 0.618464\n",
      "[900]\tvalid_0's binary_logloss: 0.614559\n",
      "[1000]\tvalid_0's binary_logloss: 0.612868\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's binary_logloss: 0.612868\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.661902\n",
      "[200]\tvalid_0's binary_logloss: 0.657512\n",
      "[300]\tvalid_0's binary_logloss: 0.65005\n",
      "[400]\tvalid_0's binary_logloss: 0.642087\n",
      "[500]\tvalid_0's binary_logloss: 0.632989\n",
      "[600]\tvalid_0's binary_logloss: 0.626948\n",
      "[700]\tvalid_0's binary_logloss: 0.621916\n",
      "[800]\tvalid_0's binary_logloss: 0.617298\n",
      "[900]\tvalid_0's binary_logloss: 0.614667\n",
      "[1000]\tvalid_0's binary_logloss: 0.613505\n",
      "Early stopping, best iteration is:\n",
      "[980]\tvalid_0's binary_logloss: 0.613027\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[79]\tvalid_0's binary_logloss: 0.626127\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.661519\n",
      "[200]\tvalid_0's binary_logloss: 0.653847\n",
      "[300]\tvalid_0's binary_logloss: 0.646215\n",
      "[400]\tvalid_0's binary_logloss: 0.635457\n",
      "[500]\tvalid_0's binary_logloss: 0.627896\n",
      "[600]\tvalid_0's binary_logloss: 0.621466\n",
      "[700]\tvalid_0's binary_logloss: 0.617459\n",
      "[800]\tvalid_0's binary_logloss: 0.61483\n",
      "Early stopping, best iteration is:\n",
      "[841]\tvalid_0's binary_logloss: 0.613846\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.66205\n",
      "[200]\tvalid_0's binary_logloss: 0.657535\n",
      "[300]\tvalid_0's binary_logloss: 0.650337\n",
      "[400]\tvalid_0's binary_logloss: 0.642526\n",
      "[500]\tvalid_0's binary_logloss: 0.633266\n",
      "[600]\tvalid_0's binary_logloss: 0.627499\n",
      "[700]\tvalid_0's binary_logloss: 0.62223\n",
      "[800]\tvalid_0's binary_logloss: 0.617877\n",
      "[900]\tvalid_0's binary_logloss: 0.614673\n",
      "[1000]\tvalid_0's binary_logloss: 0.613198\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's binary_logloss: 0.613198\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.650625\n",
      "[200]\tvalid_0's binary_logloss: 0.647334\n",
      "[300]\tvalid_0's binary_logloss: 0.64542\n",
      "Early stopping, best iteration is:\n",
      "[285]\tvalid_0's binary_logloss: 0.645039\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[76]\tvalid_0's binary_logloss: 0.63283\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.632744\n",
      "Early stopping, best iteration is:\n",
      "[137]\tvalid_0's binary_logloss: 0.625971\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.65035\n",
      "[200]\tvalid_0's binary_logloss: 0.647016\n",
      "Early stopping, best iteration is:\n",
      "[278]\tvalid_0's binary_logloss: 0.644683\n",
      "Optimized params are:\n",
      "{'lambda_l2': 0.42023632108706066, 'min_data_in_leaf': 18.68270988692906, 'num_leaves': 187.59716010015083}\n"
     ]
    }
   ],
   "source": [
    "params = lightgbm_bo(X_train, X_test, y_train, y_test,num_l=200,min_d=20,l2=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttrain's binary_logloss: 0.685778\ttrain's accuracy: 0.444444\teval's binary_logloss: 0.682346\teval's accuracy: 0.428571\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_data = lgbm.Dataset(X_train, label=y_train)\n",
    "eval_data = lgbm.Dataset(X_test, label=y_test, reference= train_data)\n",
    "evaluation_results = {}\n",
    "        \n",
    "\n",
    "cls = lgbm.train(params,\n",
    "                train_data,\n",
    "                valid_sets=[train_data, eval_data],\n",
    "                valid_names=['train', 'eval'],\n",
    "                evals_result=evaluation_results,\n",
    "                num_boost_round=1000,\n",
    "                early_stopping_rounds=20,\n",
    "                verbose_eval=100,\n",
    "                feval = feval_acc_lgbm,\n",
    "                )\n",
    "\n",
    "pred = cls.predict(X_test)\n",
    "#         acc =  [super_accuracy(pred, test_df_y, bias=b) for b in [0, 0.1, 0.2, 0.3]]\n",
    "\n",
    "#         mapping_score(evaluation_results,acc,name='lightbgm')\n",
    "\n",
    "_, ax = plt.subplots(figsize=(12, 12))\n",
    "lgbm.plot_importance(cls,\n",
    "                        ax=ax,\n",
    "                        importance_type='gain'\n",
    "                        )\n",
    "\n",
    "plt.show()\n",
    "#         mapping_score(evaluation_results,acc,name='lightbgm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "import numpy as np    \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "def ridge_logistic(train_df_x,train_df_y,test_df_x,test_df_y):\n",
    "\n",
    "    # find hps by baysian\n",
    "    cout = 0\n",
    "    alphas = [0,0.1,10,100,1000,10000]\n",
    "    for alp in range(len(alphas)-1):\n",
    "        lg_bo = BayesianOptimization(linear_ridge_eval, \n",
    "                                    {'alpha': (alphas[alp],alphas[alp+1]),\n",
    "                                    'max_iter': (3,500)\n",
    "                                    },\n",
    "                                    )\n",
    "        lg_bo.maximize(init_points=10, n_iter=50, acq='ei')\n",
    "        if cout ==0:\n",
    "            cout +=1\n",
    "            res = lg_bo\n",
    "        else:\n",
    "            if lg_bo.max['target'] > res.max['target']:\n",
    "                res = lg_bo\n",
    "\n",
    "    print('alpha is : ' + str(res.max['params']['alpha']))\n",
    "\n",
    "    # train model\n",
    "\n",
    "    model = Ridge(alpha=res.max['params']['alpha'], max_iter=int(res.max['params']['max_iter']))\n",
    "    clf = model.fit(train_df_x, list(train_df_y))\n",
    "    pred = clf.predict(test_df_x)\n",
    "   \n",
    "  \n",
    "    feature_importance = abs(np.array(clf.coef_))\n",
    "    feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "    sorted_idx = np.argsort(feature_importance)\n",
    "    pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "\n",
    "    featfig = plt.figure(figsize=(10,10))\n",
    "    featax = featfig.add_subplot(1, 1, 1)\n",
    "    featax.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "    featax.set_yticks(pos)\n",
    "    featax.set_yticklabels(np.array(test_df_x.columns)[sorted_idx], fontsize=8)\n",
    "    featax.set_xlabel('Relative Feature Importance')\n",
    "\n",
    "    plt.tight_layout() \n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import Ridge\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def linear_ridge_eval(alpha,max_iter):\n",
    "\n",
    "\n",
    "    train_df_x = pd.DataFrame(X_train)\n",
    "    train_df_y = pd.DataFrame(y_train)\n",
    "    test_df_x = pd.DataFrame(X_test)\n",
    "    test_df_y = pd.DataFrame(y_test)\n",
    "\n",
    "\n",
    "    params = {             \n",
    "            'alpha': alpha,\n",
    "            'max_iter': int(max_iter),\n",
    "            }\n",
    "\n",
    "    cls = Ridge()\n",
    "    cls.set_params(**params)\n",
    "    cls.fit(train_df_x,\n",
    "            y_train\n",
    "            )\n",
    "\n",
    "    pred = cls.predict(test_df_x)\n",
    "    score = 1 - log_loss(y_test, pred)\n",
    "    return score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |   alpha   | max_iter  |\n",
      "-------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-2.898   \u001b[0m | \u001b[0m 0.088   \u001b[0m | \u001b[0m 144.3   \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m-2.899   \u001b[0m | \u001b[0m 0.06305 \u001b[0m | \u001b[0m 139.4   \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m-2.898   \u001b[0m | \u001b[95m 0.09233 \u001b[0m | \u001b[95m 236.4   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-2.901   \u001b[0m | \u001b[0m 0.04155 \u001b[0m | \u001b[0m 7.503   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-2.899   \u001b[0m | \u001b[0m 0.06487 \u001b[0m | \u001b[0m 141.8   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m-2.899   \u001b[0m | \u001b[0m 0.06325 \u001b[0m | \u001b[0m 346.8   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m-2.903   \u001b[0m | \u001b[0m 0.01568 \u001b[0m | \u001b[0m 16.42   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m-2.901   \u001b[0m | \u001b[0m 0.03359 \u001b[0m | \u001b[0m 385.3   \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m-2.904   \u001b[0m | \u001b[0m 0.008596\u001b[0m | \u001b[0m 110.2   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m-2.902   \u001b[0m | \u001b[0m 0.03058 \u001b[0m | \u001b[0m 10.62   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m-2.899   \u001b[0m | \u001b[0m 0.06216 \u001b[0m | \u001b[0m 499.9   \u001b[0m |\n",
      "| \u001b[95m 12      \u001b[0m | \u001b[95m-2.898   \u001b[0m | \u001b[95m 0.09467 \u001b[0m | \u001b[95m 500.0   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m-2.899   \u001b[0m | \u001b[0m 0.0717  \u001b[0m | \u001b[0m 499.9   \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m-2.899   \u001b[0m | \u001b[0m 0.05966 \u001b[0m | \u001b[0m 499.9   \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m-2.899   \u001b[0m | \u001b[0m 0.06954 \u001b[0m | \u001b[0m 500.0   \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m-2.905   \u001b[0m | \u001b[0m 0.00184 \u001b[0m | \u001b[0m 499.8   \u001b[0m |\n",
      "| \u001b[95m 17      \u001b[0m | \u001b[95m-2.897   \u001b[0m | \u001b[95m 0.09842 \u001b[0m | \u001b[95m 192.8   \u001b[0m |\n",
      "| \u001b[95m 18      \u001b[0m | \u001b[95m-2.897   \u001b[0m | \u001b[95m 0.09846 \u001b[0m | \u001b[95m 299.9   \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m-2.897   \u001b[0m | \u001b[0m 0.09818 \u001b[0m | \u001b[0m 209.9   \u001b[0m |\n",
      "| \u001b[95m 20      \u001b[0m | \u001b[95m-2.897   \u001b[0m | \u001b[95m 0.09918 \u001b[0m | \u001b[95m 208.5   \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m-2.897   \u001b[0m | \u001b[0m 0.09707 \u001b[0m | \u001b[0m 207.0   \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m-2.897   \u001b[0m | \u001b[0m 0.09836 \u001b[0m | \u001b[0m 262.9   \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m-2.897   \u001b[0m | \u001b[0m 0.09805 \u001b[0m | \u001b[0m 236.5   \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m-2.897   \u001b[0m | \u001b[0m 0.0989  \u001b[0m | \u001b[0m 238.1   \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m-2.897   \u001b[0m | \u001b[0m 0.09816 \u001b[0m | \u001b[0m 209.5   \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m-2.897   \u001b[0m | \u001b[0m 0.09849 \u001b[0m | \u001b[0m 236.4   \u001b[0m |\n",
      "| \u001b[95m 27      \u001b[0m | \u001b[95m-2.897   \u001b[0m | \u001b[95m 0.09991 \u001b[0m | \u001b[95m 237.8   \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m-2.897   \u001b[0m | \u001b[0m 0.09871 \u001b[0m | \u001b[0m 207.2   \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m-2.902   \u001b[0m | \u001b[0m 0.02934 \u001b[0m | \u001b[0m 499.3   \u001b[0m |\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m-2.897   \u001b[0m | \u001b[0m 0.09778 \u001b[0m | \u001b[0m 237.6   \u001b[0m |\n",
      "| \u001b[0m 31      \u001b[0m | \u001b[0m-2.897   \u001b[0m | \u001b[0m 0.09806 \u001b[0m | \u001b[0m 236.9   \u001b[0m |\n",
      "| \u001b[0m 32      \u001b[0m | \u001b[0m-2.897   \u001b[0m | \u001b[0m 0.09921 \u001b[0m | \u001b[0m 204.7   \u001b[0m |\n",
      "| \u001b[0m 33      \u001b[0m | \u001b[0m-2.9     \u001b[0m | \u001b[0m 0.04986 \u001b[0m | \u001b[0m 3.054   \u001b[0m |\n",
      "| \u001b[0m 34      \u001b[0m | \u001b[0m-2.897   \u001b[0m | \u001b[0m 0.09958 \u001b[0m | \u001b[0m 194.3   \u001b[0m |\n",
      "| \u001b[95m 35      \u001b[0m | \u001b[95m-2.897   \u001b[0m | \u001b[95m 0.09993 \u001b[0m | \u001b[95m 194.9   \u001b[0m |\n",
      "| \u001b[0m 36      \u001b[0m | \u001b[0m-2.897   \u001b[0m | \u001b[0m 0.09919 \u001b[0m | \u001b[0m 276.8   \u001b[0m |\n",
      "| \u001b[0m 37      \u001b[0m | \u001b[0m-2.897   \u001b[0m | \u001b[0m 0.09776 \u001b[0m | \u001b[0m 197.7   \u001b[0m |\n",
      "| \u001b[0m 38      \u001b[0m | \u001b[0m-2.897   \u001b[0m | \u001b[0m 0.09812 \u001b[0m | \u001b[0m 274.5   \u001b[0m |\n",
      "| \u001b[0m 39      \u001b[0m | \u001b[0m-2.897   \u001b[0m | \u001b[0m 0.09945 \u001b[0m | \u001b[0m 201.3   \u001b[0m |\n",
      "| \u001b[0m 40      \u001b[0m | \u001b[0m-2.897   \u001b[0m | \u001b[0m 0.09982 \u001b[0m | \u001b[0m 199.8   \u001b[0m |\n",
      "| \u001b[0m 41      \u001b[0m | \u001b[0m-2.897   \u001b[0m | \u001b[0m 0.0992  \u001b[0m | \u001b[0m 274.2   \u001b[0m |\n",
      "| \u001b[0m 42      \u001b[0m | \u001b[0m-2.897   \u001b[0m | \u001b[0m 0.0962  \u001b[0m | \u001b[0m 200.0   \u001b[0m |\n",
      "| \u001b[0m 43      \u001b[0m | \u001b[0m-2.897   \u001b[0m | \u001b[0m 0.09888 \u001b[0m | \u001b[0m 270.4   \u001b[0m |\n",
      "| \u001b[0m 44      \u001b[0m | \u001b[0m-2.897   \u001b[0m | \u001b[0m 0.09892 \u001b[0m | \u001b[0m 203.8   \u001b[0m |\n",
      "| \u001b[0m 45      \u001b[0m | \u001b[0m-2.897   \u001b[0m | \u001b[0m 0.0992  \u001b[0m | \u001b[0m 265.8   \u001b[0m |\n",
      "| \u001b[0m 46      \u001b[0m | \u001b[0m-2.897   \u001b[0m | \u001b[0m 0.09991 \u001b[0m | \u001b[0m 206.0   \u001b[0m |\n",
      "| \u001b[0m 47      \u001b[0m | \u001b[0m-2.902   \u001b[0m | \u001b[0m 0.03141 \u001b[0m | \u001b[0m 3.407   \u001b[0m |\n",
      "| \u001b[0m 48      \u001b[0m | \u001b[0m-2.897   \u001b[0m | \u001b[0m 0.09755 \u001b[0m | \u001b[0m 230.2   \u001b[0m |\n",
      "| \u001b[0m 49      \u001b[0m | \u001b[0m-2.897   \u001b[0m | \u001b[0m 0.09864 \u001b[0m | \u001b[0m 227.8   \u001b[0m |\n",
      "| \u001b[0m 50      \u001b[0m | \u001b[0m-2.897   \u001b[0m | \u001b[0m 0.0978  \u001b[0m | \u001b[0m 228.7   \u001b[0m |\n",
      "| \u001b[0m 51      \u001b[0m | \u001b[0m-2.901   \u001b[0m | \u001b[0m 0.04212 \u001b[0m | \u001b[0m 499.7   \u001b[0m |\n",
      "| \u001b[0m 52      \u001b[0m | \u001b[0m-2.897   \u001b[0m | \u001b[0m 0.0987  \u001b[0m | \u001b[0m 223.9   \u001b[0m |\n",
      "| \u001b[0m 53      \u001b[0m | \u001b[0m-2.904   \u001b[0m | \u001b[0m 0.01021 \u001b[0m | \u001b[0m 3.498   \u001b[0m |\n",
      "| \u001b[0m 54      \u001b[0m | \u001b[0m-2.897   \u001b[0m | \u001b[0m 0.09646 \u001b[0m | \u001b[0m 235.3   \u001b[0m |\n",
      "| \u001b[0m 55      \u001b[0m | \u001b[0m-2.897   \u001b[0m | \u001b[0m 0.0983  \u001b[0m | \u001b[0m 238.2   \u001b[0m |\n",
      "| \u001b[0m 56      \u001b[0m | \u001b[0m-2.897   \u001b[0m | \u001b[0m 0.09793 \u001b[0m | \u001b[0m 234.6   \u001b[0m |\n",
      "| \u001b[0m 57      \u001b[0m | \u001b[0m-2.897   \u001b[0m | \u001b[0m 0.0966  \u001b[0m | \u001b[0m 236.7   \u001b[0m |\n",
      "| \u001b[0m 58      \u001b[0m | \u001b[0m-2.898   \u001b[0m | \u001b[0m 0.09363 \u001b[0m | \u001b[0m 236.8   \u001b[0m |\n",
      "| \u001b[0m 59      \u001b[0m | \u001b[0m-2.897   \u001b[0m | \u001b[0m 0.0991  \u001b[0m | \u001b[0m 235.8   \u001b[0m |\n",
      "| \u001b[0m 60      \u001b[0m | \u001b[0m-2.897   \u001b[0m | \u001b[0m 0.09862 \u001b[0m | \u001b[0m 237.4   \u001b[0m |\n",
      "=================================================\n",
      "|   iter    |  target   |   alpha   | max_iter  |\n",
      "-------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.2357  \u001b[0m | \u001b[0m 8.859   \u001b[0m | \u001b[0m 238.2   \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.2364  \u001b[0m | \u001b[95m 8.951   \u001b[0m | \u001b[95m 101.5   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-1.394   \u001b[0m | \u001b[0m 1.39    \u001b[0m | \u001b[0m 186.7   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.2179  \u001b[0m | \u001b[0m 6.799   \u001b[0m | \u001b[0m 143.6   \u001b[0m |\n",
      "| \u001b[95m 5       \u001b[0m | \u001b[95m 0.2384  \u001b[0m | \u001b[95m 9.226   \u001b[0m | \u001b[95m 372.2   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.1817  \u001b[0m | \u001b[0m 4.318   \u001b[0m | \u001b[0m 153.3   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.2055  \u001b[0m | \u001b[0m 5.746   \u001b[0m | \u001b[0m 416.3   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m-2.894   \u001b[0m | \u001b[0m 0.3283  \u001b[0m | \u001b[0m 283.1   \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.1963  \u001b[0m | \u001b[0m 5.112   \u001b[0m | \u001b[0m 89.67   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.2268  \u001b[0m | \u001b[0m 7.731   \u001b[0m | \u001b[0m 490.2   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.2297  \u001b[0m | \u001b[0m 8.076   \u001b[0m | \u001b[0m 372.1   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.2322  \u001b[0m | \u001b[0m 8.389   \u001b[0m | \u001b[0m 455.9   \u001b[0m |\n",
      "| \u001b[95m 13      \u001b[0m | \u001b[95m 0.2434  \u001b[0m | \u001b[95m 10.0    \u001b[0m | \u001b[95m 40.89   \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m-1.468   \u001b[0m | \u001b[0m 1.186   \u001b[0m | \u001b[0m 4.799   \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.2434  \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 60.15   \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.2434  \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 392.9   \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m-2.897   \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 473.1   \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.2434  \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 447.7   \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 0.2432  \u001b[0m | \u001b[0m 9.958   \u001b[0m | \u001b[0m 498.3   \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.1286  \u001b[0m | \u001b[0m 2.776   \u001b[0m | \u001b[0m 50.41   \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m-2.897   \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 402.9   \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 0.2434  \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 385.5   \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m 0.2333  \u001b[0m | \u001b[0m 8.53    \u001b[0m | \u001b[0m 422.4   \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m 0.2434  \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 49.7    \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m-2.897   \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 98.58   \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m 0.2247  \u001b[0m | \u001b[0m 7.502   \u001b[0m | \u001b[0m 55.19   \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m 0.236   \u001b[0m | \u001b[0m 8.9     \u001b[0m | \u001b[0m 87.05   \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m 0.2431  \u001b[0m | \u001b[0m 9.951   \u001b[0m | \u001b[0m 93.33   \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m 0.2434  \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 106.8   \u001b[0m |\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m 0.1314  \u001b[0m | \u001b[0m 2.823   \u001b[0m | \u001b[0m 496.4   \u001b[0m |\n",
      "| \u001b[0m 31      \u001b[0m | \u001b[0m 0.1558  \u001b[0m | \u001b[0m 3.37    \u001b[0m | \u001b[0m 449.6   \u001b[0m |\n",
      "| \u001b[0m 32      \u001b[0m | \u001b[0m 0.06906 \u001b[0m | \u001b[0m 2.175   \u001b[0m | \u001b[0m 80.79   \u001b[0m |\n",
      "| \u001b[0m 33      \u001b[0m | \u001b[0m-2.894   \u001b[0m | \u001b[0m 0.7532  \u001b[0m | \u001b[0m 423.3   \u001b[0m |\n",
      "| \u001b[0m 34      \u001b[0m | \u001b[0m 0.2434  \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 418.0   \u001b[0m |\n",
      "| \u001b[0m 35      \u001b[0m | \u001b[0m 0.2355  \u001b[0m | \u001b[0m 8.822   \u001b[0m | \u001b[0m 378.3   \u001b[0m |\n",
      "| \u001b[0m 36      \u001b[0m | \u001b[0m-2.895   \u001b[0m | \u001b[0m 1.099   \u001b[0m | \u001b[0m 147.2   \u001b[0m |\n",
      "| \u001b[0m 37      \u001b[0m | \u001b[0m 0.2421  \u001b[0m | \u001b[0m 9.794   \u001b[0m | \u001b[0m 141.9   \u001b[0m |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 38      \u001b[0m | \u001b[0m 0.2144  \u001b[0m | \u001b[0m 6.479   \u001b[0m | \u001b[0m 155.9   \u001b[0m |\n",
      "| \u001b[0m 39      \u001b[0m | \u001b[0m 0.2288  \u001b[0m | \u001b[0m 7.975   \u001b[0m | \u001b[0m 451.7   \u001b[0m |\n",
      "| \u001b[0m 40      \u001b[0m | \u001b[0m 0.2295  \u001b[0m | \u001b[0m 8.05    \u001b[0m | \u001b[0m 494.7   \u001b[0m |\n",
      "| \u001b[0m 41      \u001b[0m | \u001b[0m 0.2434  \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 146.8   \u001b[0m |\n",
      "| \u001b[0m 42      \u001b[0m | \u001b[0m 0.2148  \u001b[0m | \u001b[0m 6.514   \u001b[0m | \u001b[0m 44.76   \u001b[0m |\n",
      "| \u001b[0m 43      \u001b[0m | \u001b[0m 0.1823  \u001b[0m | \u001b[0m 4.344   \u001b[0m | \u001b[0m 383.5   \u001b[0m |\n",
      "| \u001b[0m 44      \u001b[0m | \u001b[0m 0.2182  \u001b[0m | \u001b[0m 6.826   \u001b[0m | \u001b[0m 389.0   \u001b[0m |\n",
      "| \u001b[0m 45      \u001b[0m | \u001b[0m 0.1667  \u001b[0m | \u001b[0m 3.709   \u001b[0m | \u001b[0m 61.24   \u001b[0m |\n",
      "| \u001b[0m 46      \u001b[0m | \u001b[0m 0.2303  \u001b[0m | \u001b[0m 8.154   \u001b[0m | \u001b[0m 67.61   \u001b[0m |\n",
      "| \u001b[0m 47      \u001b[0m | \u001b[0m-0.02497 \u001b[0m | \u001b[0m 1.928   \u001b[0m | \u001b[0m 70.54   \u001b[0m |\n",
      "| \u001b[0m 48      \u001b[0m | \u001b[0m 0.2434  \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 231.7   \u001b[0m |\n",
      "| \u001b[0m 49      \u001b[0m | \u001b[0m 0.1677  \u001b[0m | \u001b[0m 3.745   \u001b[0m | \u001b[0m 233.8   \u001b[0m |\n",
      "| \u001b[0m 50      \u001b[0m | \u001b[0m 0.2432  \u001b[0m | \u001b[0m 9.965   \u001b[0m | \u001b[0m 76.13   \u001b[0m |\n",
      "| \u001b[0m 51      \u001b[0m | \u001b[0m 0.1767  \u001b[0m | \u001b[0m 4.094   \u001b[0m | \u001b[0m 36.55   \u001b[0m |\n",
      "| \u001b[0m 52      \u001b[0m | \u001b[0m 0.2434  \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 32.3    \u001b[0m |\n",
      "| \u001b[0m 53      \u001b[0m | \u001b[0m 0.1712  \u001b[0m | \u001b[0m 3.872   \u001b[0m | \u001b[0m 27.91   \u001b[0m |\n",
      "| \u001b[0m 54      \u001b[0m | \u001b[0m 0.2051  \u001b[0m | \u001b[0m 5.712   \u001b[0m | \u001b[0m 224.2   \u001b[0m |\n",
      "| \u001b[0m 55      \u001b[0m | \u001b[0m 0.1255  \u001b[0m | \u001b[0m 2.725   \u001b[0m | \u001b[0m 244.2   \u001b[0m |\n",
      "| \u001b[0m 56      \u001b[0m | \u001b[0m 0.2434  \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 247.4   \u001b[0m |\n",
      "| \u001b[0m 57      \u001b[0m | \u001b[0m 0.1977  \u001b[0m | \u001b[0m 5.198   \u001b[0m | \u001b[0m 253.9   \u001b[0m |\n",
      "| \u001b[0m 58      \u001b[0m | \u001b[0m 0.2434  \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 363.2   \u001b[0m |\n",
      "| \u001b[0m 59      \u001b[0m | \u001b[0m 0.006399\u001b[0m | \u001b[0m 1.969   \u001b[0m | \u001b[0m 363.2   \u001b[0m |\n",
      "| \u001b[0m 60      \u001b[0m | \u001b[0m 0.2434  \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 354.5   \u001b[0m |\n",
      "=================================================\n",
      "|   iter    |  target   |   alpha   | max_iter  |\n",
      "-------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.2779  \u001b[0m | \u001b[0m 18.22   \u001b[0m | \u001b[0m 158.5   \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.2771  \u001b[0m | \u001b[0m 17.94   \u001b[0m | \u001b[0m 413.0   \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.3027  \u001b[0m | \u001b[95m 35.43   \u001b[0m | \u001b[95m 332.2   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.3024  \u001b[0m | \u001b[0m 35.07   \u001b[0m | \u001b[0m 250.9   \u001b[0m |\n",
      "| \u001b[95m 5       \u001b[0m | \u001b[95m 0.3178  \u001b[0m | \u001b[95m 71.17   \u001b[0m | \u001b[95m 182.6   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.2929  \u001b[0m | \u001b[0m 25.71   \u001b[0m | \u001b[0m 133.2   \u001b[0m |\n",
      "| \u001b[95m 7       \u001b[0m | \u001b[95m 0.3195  \u001b[0m | \u001b[95m 79.16   \u001b[0m | \u001b[95m 270.8   \u001b[0m |\n",
      "| \u001b[95m 8       \u001b[0m | \u001b[95m 0.3212  \u001b[0m | \u001b[95m 88.37   \u001b[0m | \u001b[95m 192.4   \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.2995  \u001b[0m | \u001b[0m 31.73   \u001b[0m | \u001b[0m 18.13   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.3171  \u001b[0m | \u001b[0m 68.42   \u001b[0m | \u001b[0m 305.6   \u001b[0m |\n",
      "| \u001b[95m 11      \u001b[0m | \u001b[95m 0.3229  \u001b[0m | \u001b[95m 99.57   \u001b[0m | \u001b[95m 52.9    \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.3226  \u001b[0m | \u001b[0m 97.16   \u001b[0m | \u001b[0m 54.86   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.3227  \u001b[0m | \u001b[0m 97.91   \u001b[0m | \u001b[0m 499.8   \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.3219  \u001b[0m | \u001b[0m 92.8    \u001b[0m | \u001b[0m 106.7   \u001b[0m |\n",
      "| \u001b[95m 15      \u001b[0m | \u001b[95m 0.323   \u001b[0m | \u001b[95m 99.95   \u001b[0m | \u001b[95m 425.9   \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.3227  \u001b[0m | \u001b[0m 97.72   \u001b[0m | \u001b[0m 5.129   \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 0.3229  \u001b[0m | \u001b[0m 99.61   \u001b[0m | \u001b[0m 473.1   \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.3201  \u001b[0m | \u001b[0m 82.06   \u001b[0m | \u001b[0m 58.96   \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 0.323   \u001b[0m | \u001b[0m 99.92   \u001b[0m | \u001b[0m 361.0   \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.3229  \u001b[0m | \u001b[0m 99.59   \u001b[0m | \u001b[0m 387.1   \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m 0.3229  \u001b[0m | \u001b[0m 99.74   \u001b[0m | \u001b[0m 149.9   \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 0.323   \u001b[0m | \u001b[0m 99.88   \u001b[0m | \u001b[0m 486.6   \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m 0.3229  \u001b[0m | \u001b[0m 99.67   \u001b[0m | \u001b[0m 498.2   \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m 0.3229  \u001b[0m | \u001b[0m 99.65   \u001b[0m | \u001b[0m 499.9   \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 0.323   \u001b[0m | \u001b[0m 99.85   \u001b[0m | \u001b[0m 17.05   \u001b[0m |\n",
      "| \u001b[95m 26      \u001b[0m | \u001b[95m 0.323   \u001b[0m | \u001b[95m 100.0   \u001b[0m | \u001b[95m 386.0   \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m 0.323   \u001b[0m | \u001b[0m 99.84   \u001b[0m | \u001b[0m 445.8   \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m 0.3228  \u001b[0m | \u001b[0m 99.03   \u001b[0m | \u001b[0m 3.73    \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m 0.323   \u001b[0m | \u001b[0m 99.97   \u001b[0m | \u001b[0m 393.5   \u001b[0m |\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m 0.323   \u001b[0m | \u001b[0m 99.84   \u001b[0m | \u001b[0m 498.2   \u001b[0m |\n",
      "| \u001b[0m 31      \u001b[0m | \u001b[0m 0.323   \u001b[0m | \u001b[0m 99.93   \u001b[0m | \u001b[0m 219.9   \u001b[0m |\n",
      "| \u001b[0m 32      \u001b[0m | \u001b[0m 0.323   \u001b[0m | \u001b[0m 99.99   \u001b[0m | \u001b[0m 176.5   \u001b[0m |\n",
      "| \u001b[0m 33      \u001b[0m | \u001b[0m 0.323   \u001b[0m | \u001b[0m 99.89   \u001b[0m | \u001b[0m 472.7   \u001b[0m |\n",
      "| \u001b[0m 34      \u001b[0m | \u001b[0m 0.323   \u001b[0m | \u001b[0m 99.95   \u001b[0m | \u001b[0m 3.492   \u001b[0m |\n",
      "| \u001b[0m 35      \u001b[0m | \u001b[0m 0.323   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 421.8   \u001b[0m |\n",
      "| \u001b[0m 36      \u001b[0m | \u001b[0m 0.3229  \u001b[0m | \u001b[0m 99.78   \u001b[0m | \u001b[0m 143.1   \u001b[0m |\n",
      "| \u001b[0m 37      \u001b[0m | \u001b[0m 0.323   \u001b[0m | \u001b[0m 99.96   \u001b[0m | \u001b[0m 325.1   \u001b[0m |\n",
      "| \u001b[0m 38      \u001b[0m | \u001b[0m 0.323   \u001b[0m | \u001b[0m 99.95   \u001b[0m | \u001b[0m 361.1   \u001b[0m |\n",
      "| \u001b[0m 39      \u001b[0m | \u001b[0m 0.323   \u001b[0m | \u001b[0m 99.87   \u001b[0m | \u001b[0m 173.1   \u001b[0m |\n",
      "| \u001b[0m 40      \u001b[0m | \u001b[0m 0.323   \u001b[0m | \u001b[0m 99.85   \u001b[0m | \u001b[0m 351.0   \u001b[0m |\n",
      "| \u001b[0m 41      \u001b[0m | \u001b[0m 0.323   \u001b[0m | \u001b[0m 99.87   \u001b[0m | \u001b[0m 201.3   \u001b[0m |\n",
      "| \u001b[0m 42      \u001b[0m | \u001b[0m 0.3229  \u001b[0m | \u001b[0m 99.68   \u001b[0m | \u001b[0m 240.0   \u001b[0m |\n",
      "| \u001b[0m 43      \u001b[0m | \u001b[0m 0.323   \u001b[0m | \u001b[0m 99.97   \u001b[0m | \u001b[0m 21.73   \u001b[0m |\n",
      "| \u001b[0m 44      \u001b[0m | \u001b[0m 0.323   \u001b[0m | \u001b[0m 99.96   \u001b[0m | \u001b[0m 472.6   \u001b[0m |\n",
      "| \u001b[0m 45      \u001b[0m | \u001b[0m 0.323   \u001b[0m | \u001b[0m 99.99   \u001b[0m | \u001b[0m 347.3   \u001b[0m |\n",
      "| \u001b[0m 46      \u001b[0m | \u001b[0m 0.323   \u001b[0m | \u001b[0m 99.85   \u001b[0m | \u001b[0m 3.022   \u001b[0m |\n",
      "| \u001b[0m 47      \u001b[0m | \u001b[0m 0.323   \u001b[0m | \u001b[0m 99.96   \u001b[0m | \u001b[0m 206.1   \u001b[0m |\n",
      "| \u001b[0m 48      \u001b[0m | \u001b[0m 0.323   \u001b[0m | \u001b[0m 99.96   \u001b[0m | \u001b[0m 493.2   \u001b[0m |\n",
      "| \u001b[0m 49      \u001b[0m | \u001b[0m 0.323   \u001b[0m | \u001b[0m 99.96   \u001b[0m | \u001b[0m 142.5   \u001b[0m |\n",
      "| \u001b[0m 50      \u001b[0m | \u001b[0m 0.323   \u001b[0m | \u001b[0m 99.96   \u001b[0m | \u001b[0m 424.5   \u001b[0m |\n",
      "| \u001b[0m 51      \u001b[0m | \u001b[0m 0.323   \u001b[0m | \u001b[0m 99.98   \u001b[0m | \u001b[0m 494.5   \u001b[0m |\n",
      "| \u001b[0m 52      \u001b[0m | \u001b[0m 0.323   \u001b[0m | \u001b[0m 99.91   \u001b[0m | \u001b[0m 378.2   \u001b[0m |\n",
      "| \u001b[0m 53      \u001b[0m | \u001b[0m 0.323   \u001b[0m | \u001b[0m 99.95   \u001b[0m | \u001b[0m 499.3   \u001b[0m |\n",
      "| \u001b[0m 54      \u001b[0m | \u001b[0m 0.323   \u001b[0m | \u001b[0m 99.86   \u001b[0m | \u001b[0m 170.0   \u001b[0m |\n",
      "| \u001b[0m 55      \u001b[0m | \u001b[0m 0.323   \u001b[0m | \u001b[0m 99.99   \u001b[0m | \u001b[0m 463.3   \u001b[0m |\n",
      "| \u001b[0m 56      \u001b[0m | \u001b[0m 0.323   \u001b[0m | \u001b[0m 99.97   \u001b[0m | \u001b[0m 287.1   \u001b[0m |\n",
      "| \u001b[0m 57      \u001b[0m | \u001b[0m 0.323   \u001b[0m | \u001b[0m 99.83   \u001b[0m | \u001b[0m 232.6   \u001b[0m |\n",
      "| \u001b[0m 58      \u001b[0m | \u001b[0m 0.323   \u001b[0m | \u001b[0m 99.93   \u001b[0m | \u001b[0m 322.1   \u001b[0m |\n",
      "| \u001b[0m 59      \u001b[0m | \u001b[0m 0.3229  \u001b[0m | \u001b[0m 99.76   \u001b[0m | \u001b[0m 105.0   \u001b[0m |\n",
      "| \u001b[95m 60      \u001b[0m | \u001b[95m 0.323   \u001b[0m | \u001b[95m 100.0   \u001b[0m | \u001b[95m 337.9   \u001b[0m |\n",
      "=================================================\n",
      "|   iter    |  target   |   alpha   | max_iter  |\n",
      "-------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.3406  \u001b[0m | \u001b[0m 488.3   \u001b[0m | \u001b[0m 178.0   \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.3445  \u001b[0m | \u001b[95m 756.5   \u001b[0m | \u001b[95m 275.1   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.3257  \u001b[0m | \u001b[0m 122.5   \u001b[0m | \u001b[0m 266.3   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.3319  \u001b[0m | \u001b[0m 207.7   \u001b[0m | \u001b[0m 74.43   \u001b[0m |\n",
      "| \u001b[95m 5       \u001b[0m | \u001b[95m 0.3468  \u001b[0m | \u001b[95m 995.4   \u001b[0m | \u001b[95m 97.57   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.3455  \u001b[0m | \u001b[0m 845.8   \u001b[0m | \u001b[0m 19.09   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.3455  \u001b[0m | \u001b[0m 845.3   \u001b[0m | \u001b[0m 210.5   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.3456  \u001b[0m | \u001b[0m 864.2   \u001b[0m | \u001b[0m 78.34   \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.3452  \u001b[0m | \u001b[0m 821.6   \u001b[0m | \u001b[0m 29.87   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.3414  \u001b[0m | \u001b[0m 530.0   \u001b[0m | \u001b[0m 116.6   \u001b[0m |\n",
      "| \u001b[95m 11      \u001b[0m | \u001b[95m 0.3468  \u001b[0m | \u001b[95m 996.0   \u001b[0m | \u001b[95m 497.8   \u001b[0m |\n",
      "| \u001b[95m 12      \u001b[0m | \u001b[95m 0.3468  \u001b[0m | \u001b[95m 999.8   \u001b[0m | \u001b[95m 377.9   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.3468  \u001b[0m | \u001b[0m 999.0   \u001b[0m | \u001b[0m 492.4   \u001b[0m |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.3468  \u001b[0m | \u001b[0m 999.7   \u001b[0m | \u001b[0m 292.4   \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.3468  \u001b[0m | \u001b[0m 999.5   \u001b[0m | \u001b[0m 329.2   \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.3468  \u001b[0m | \u001b[0m 996.3   \u001b[0m | \u001b[0m 498.2   \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 0.3468  \u001b[0m | \u001b[0m 999.5   \u001b[0m | \u001b[0m 297.2   \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.3468  \u001b[0m | \u001b[0m 999.1   \u001b[0m | \u001b[0m 488.9   \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 0.3468  \u001b[0m | \u001b[0m 999.7   \u001b[0m | \u001b[0m 300.2   \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.3468  \u001b[0m | \u001b[0m 999.2   \u001b[0m | \u001b[0m 497.2   \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m 0.3468  \u001b[0m | \u001b[0m 999.7   \u001b[0m | \u001b[0m 326.6   \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 0.3468  \u001b[0m | \u001b[0m 999.5   \u001b[0m | \u001b[0m 309.6   \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m 0.3468  \u001b[0m | \u001b[0m 999.7   \u001b[0m | \u001b[0m 306.5   \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m 0.3468  \u001b[0m | \u001b[0m 998.1   \u001b[0m | \u001b[0m 481.7   \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 0.3468  \u001b[0m | \u001b[0m 999.3   \u001b[0m | \u001b[0m 316.2   \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m 0.3468  \u001b[0m | \u001b[0m 999.2   \u001b[0m | \u001b[0m 338.9   \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m 0.3468  \u001b[0m | \u001b[0m 999.4   \u001b[0m | \u001b[0m 499.7   \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m 0.3468  \u001b[0m | \u001b[0m 998.5   \u001b[0m | \u001b[0m 497.2   \u001b[0m |\n",
      "| \u001b[95m 29      \u001b[0m | \u001b[95m 0.3468  \u001b[0m | \u001b[95m 999.8   \u001b[0m | \u001b[95m 345.1   \u001b[0m |\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m 0.3468  \u001b[0m | \u001b[0m 999.4   \u001b[0m | \u001b[0m 333.5   \u001b[0m |\n",
      "| \u001b[0m 31      \u001b[0m | \u001b[0m 0.3468  \u001b[0m | \u001b[0m 999.7   \u001b[0m | \u001b[0m 494.4   \u001b[0m |\n",
      "| \u001b[0m 32      \u001b[0m | \u001b[0m 0.3468  \u001b[0m | \u001b[0m 997.3   \u001b[0m | \u001b[0m 7.039   \u001b[0m |\n",
      "| \u001b[95m 33      \u001b[0m | \u001b[95m 0.3468  \u001b[0m | \u001b[95m 999.9   \u001b[0m | \u001b[95m 295.8   \u001b[0m |\n",
      "| \u001b[0m 34      \u001b[0m | \u001b[0m 0.3468  \u001b[0m | \u001b[0m 998.7   \u001b[0m | \u001b[0m 14.75   \u001b[0m |\n",
      "| \u001b[0m 35      \u001b[0m | \u001b[0m 0.3468  \u001b[0m | \u001b[0m 999.0   \u001b[0m | \u001b[0m 279.4   \u001b[0m |\n",
      "| \u001b[95m 36      \u001b[0m | \u001b[95m 0.3468  \u001b[0m | \u001b[95m 1e+03   \u001b[0m | \u001b[95m 295.9   \u001b[0m |\n",
      "| \u001b[0m 37      \u001b[0m | \u001b[0m 0.3468  \u001b[0m | \u001b[0m 998.1   \u001b[0m | \u001b[0m 4.971   \u001b[0m |\n",
      "| \u001b[0m 38      \u001b[0m | \u001b[0m 0.3468  \u001b[0m | \u001b[0m 998.9   \u001b[0m | \u001b[0m 281.7   \u001b[0m |\n",
      "| \u001b[95m 39      \u001b[0m | \u001b[95m 0.3468  \u001b[0m | \u001b[95m 1e+03   \u001b[0m | \u001b[95m 498.2   \u001b[0m |\n",
      "| \u001b[0m 40      \u001b[0m | \u001b[0m 0.3468  \u001b[0m | \u001b[0m 999.4   \u001b[0m | \u001b[0m 274.6   \u001b[0m |\n",
      "| \u001b[0m 41      \u001b[0m | \u001b[0m 0.3468  \u001b[0m | \u001b[0m 999.5   \u001b[0m | \u001b[0m 285.2   \u001b[0m |\n",
      "| \u001b[0m 42      \u001b[0m | \u001b[0m 0.3468  \u001b[0m | \u001b[0m 999.2   \u001b[0m | \u001b[0m 11.51   \u001b[0m |\n",
      "| \u001b[0m 43      \u001b[0m | \u001b[0m 0.3468  \u001b[0m | \u001b[0m 999.4   \u001b[0m | \u001b[0m 496.2   \u001b[0m |\n",
      "| \u001b[0m 44      \u001b[0m | \u001b[0m 0.3468  \u001b[0m | \u001b[0m 999.2   \u001b[0m | \u001b[0m 5.841   \u001b[0m |\n",
      "| \u001b[0m 45      \u001b[0m | \u001b[0m 0.3468  \u001b[0m | \u001b[0m 999.8   \u001b[0m | \u001b[0m 237.6   \u001b[0m |\n",
      "| \u001b[0m 46      \u001b[0m | \u001b[0m 0.3468  \u001b[0m | \u001b[0m 1e+03   \u001b[0m | \u001b[0m 256.2   \u001b[0m |\n",
      "| \u001b[0m 47      \u001b[0m | \u001b[0m 0.3468  \u001b[0m | \u001b[0m 999.2   \u001b[0m | \u001b[0m 257.8   \u001b[0m |\n",
      "| \u001b[0m 48      \u001b[0m | \u001b[0m 0.3468  \u001b[0m | \u001b[0m 997.3   \u001b[0m | \u001b[0m 6.316   \u001b[0m |\n",
      "| \u001b[0m 49      \u001b[0m | \u001b[0m 0.3468  \u001b[0m | \u001b[0m 999.9   \u001b[0m | \u001b[0m 266.4   \u001b[0m |\n",
      "| \u001b[0m 50      \u001b[0m | \u001b[0m 0.3468  \u001b[0m | \u001b[0m 999.8   \u001b[0m | \u001b[0m 483.8   \u001b[0m |\n",
      "| \u001b[0m 51      \u001b[0m | \u001b[0m 0.3468  \u001b[0m | \u001b[0m 999.8   \u001b[0m | \u001b[0m 490.9   \u001b[0m |\n",
      "| \u001b[0m 52      \u001b[0m | \u001b[0m 0.3468  \u001b[0m | \u001b[0m 998.8   \u001b[0m | \u001b[0m 273.8   \u001b[0m |\n",
      "| \u001b[0m 53      \u001b[0m | \u001b[0m 0.3468  \u001b[0m | \u001b[0m 999.3   \u001b[0m | \u001b[0m 9.079   \u001b[0m |\n",
      "| \u001b[0m 54      \u001b[0m | \u001b[0m 0.3468  \u001b[0m | \u001b[0m 999.9   \u001b[0m | \u001b[0m 292.0   \u001b[0m |\n",
      "| \u001b[0m 55      \u001b[0m | \u001b[0m 0.3468  \u001b[0m | \u001b[0m 999.9   \u001b[0m | \u001b[0m 301.8   \u001b[0m |\n",
      "| \u001b[95m 56      \u001b[0m | \u001b[95m 0.3468  \u001b[0m | \u001b[95m 1e+03   \u001b[0m | \u001b[95m 246.2   \u001b[0m |\n",
      "| \u001b[0m 57      \u001b[0m | \u001b[0m 0.3468  \u001b[0m | \u001b[0m 999.9   \u001b[0m | \u001b[0m 489.1   \u001b[0m |\n",
      "| \u001b[0m 58      \u001b[0m | \u001b[0m 0.3468  \u001b[0m | \u001b[0m 999.8   \u001b[0m | \u001b[0m 485.9   \u001b[0m |\n",
      "| \u001b[0m 59      \u001b[0m | \u001b[0m 0.3468  \u001b[0m | \u001b[0m 999.9   \u001b[0m | \u001b[0m 254.1   \u001b[0m |\n",
      "| \u001b[0m 60      \u001b[0m | \u001b[0m 0.3468  \u001b[0m | \u001b[0m 999.0   \u001b[0m | \u001b[0m 15.78   \u001b[0m |\n",
      "=================================================\n",
      "|   iter    |  target   |   alpha   | max_iter  |\n",
      "-------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 5.51e+03\u001b[0m | \u001b[0m 6.025   \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 5.356e+0\u001b[0m | \u001b[0m 192.6   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 6.714e+0\u001b[0m | \u001b[0m 95.22   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.3534  \u001b[0m | \u001b[0m 3.339e+0\u001b[0m | \u001b[0m 398.8   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.354   \u001b[0m | \u001b[0m 6.911e+0\u001b[0m | \u001b[0m 446.8   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.354   \u001b[0m | \u001b[0m 4.409e+0\u001b[0m | \u001b[0m 235.8   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.3538  \u001b[0m | \u001b[0m 7.997e+0\u001b[0m | \u001b[0m 98.64   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.3538  \u001b[0m | \u001b[0m 4.035e+0\u001b[0m | \u001b[0m 151.4   \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.3521  \u001b[0m | \u001b[0m 2.362e+0\u001b[0m | \u001b[0m 376.4   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.3525  \u001b[0m | \u001b[0m 2.584e+0\u001b[0m | \u001b[0m 33.47   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.3533  \u001b[0m | \u001b[0m 9.944e+0\u001b[0m | \u001b[0m 497.3   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 6.305e+0\u001b[0m | \u001b[0m 499.0   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 6.361e+0\u001b[0m | \u001b[0m 496.2   \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 5.384e+0\u001b[0m | \u001b[0m 214.2   \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 6.651e+0\u001b[0m | \u001b[0m 4.472   \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 6.414e+0\u001b[0m | \u001b[0m 498.9   \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 6.609e+0\u001b[0m | \u001b[0m 4.296   \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 6.265e+0\u001b[0m | \u001b[0m 496.2   \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 6.615e+0\u001b[0m | \u001b[0m 3.739   \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 6.579e+0\u001b[0m | \u001b[0m 499.0   \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 6.56e+03\u001b[0m | \u001b[0m 6.897   \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 6.405e+0\u001b[0m | \u001b[0m 496.1   \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 6.658e+0\u001b[0m | \u001b[0m 4.277   \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 6.337e+0\u001b[0m | \u001b[0m 498.1   \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 6.408e+0\u001b[0m | \u001b[0m 499.4   \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 6.449e+0\u001b[0m | \u001b[0m 3.629   \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 6.393e+0\u001b[0m | \u001b[0m 499.6   \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 6.474e+0\u001b[0m | \u001b[0m 3.251   \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 6.563e+0\u001b[0m | \u001b[0m 498.4   \u001b[0m |\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 6.658e+0\u001b[0m | \u001b[0m 4.948   \u001b[0m |\n",
      "| \u001b[0m 31      \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 6.384e+0\u001b[0m | \u001b[0m 498.3   \u001b[0m |\n",
      "| \u001b[0m 32      \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 6.419e+0\u001b[0m | \u001b[0m 5.207   \u001b[0m |\n",
      "| \u001b[0m 33      \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 6.495e+0\u001b[0m | \u001b[0m 498.5   \u001b[0m |\n",
      "| \u001b[0m 34      \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 6.339e+0\u001b[0m | \u001b[0m 4.026   \u001b[0m |\n",
      "| \u001b[0m 35      \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 6.334e+0\u001b[0m | \u001b[0m 499.6   \u001b[0m |\n",
      "| \u001b[0m 36      \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 6.741e+0\u001b[0m | \u001b[0m 5.024   \u001b[0m |\n",
      "| \u001b[0m 37      \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 6.506e+0\u001b[0m | \u001b[0m 498.6   \u001b[0m |\n",
      "| \u001b[0m 38      \u001b[0m | \u001b[0m 0.3533  \u001b[0m | \u001b[0m 9.97e+03\u001b[0m | \u001b[0m 3.113   \u001b[0m |\n",
      "| \u001b[0m 39      \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 6.448e+0\u001b[0m | \u001b[0m 8.061   \u001b[0m |\n",
      "| \u001b[0m 40      \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 6.347e+0\u001b[0m | \u001b[0m 3.21    \u001b[0m |\n",
      "| \u001b[0m 41      \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 6.673e+0\u001b[0m | \u001b[0m 497.1   \u001b[0m |\n",
      "| \u001b[0m 42      \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 6.488e+0\u001b[0m | \u001b[0m 3.254   \u001b[0m |\n",
      "| \u001b[0m 43      \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 6.402e+0\u001b[0m | \u001b[0m 498.8   \u001b[0m |\n",
      "| \u001b[0m 44      \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 6.331e+0\u001b[0m | \u001b[0m 5.4     \u001b[0m |\n",
      "| \u001b[0m 45      \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 6.46e+03\u001b[0m | \u001b[0m 498.5   \u001b[0m |\n",
      "| \u001b[0m 46      \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 6.233e+0\u001b[0m | \u001b[0m 4.269   \u001b[0m |\n",
      "| \u001b[0m 47      \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 6.241e+0\u001b[0m | \u001b[0m 4.06    \u001b[0m |\n",
      "| \u001b[0m 48      \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 6.376e+0\u001b[0m | \u001b[0m 499.8   \u001b[0m |\n",
      "| \u001b[0m 49      \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 6.507e+0\u001b[0m | \u001b[0m 3.609   \u001b[0m |\n",
      "| \u001b[0m 50      \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 6.526e+0\u001b[0m | \u001b[0m 497.2   \u001b[0m |\n",
      "| \u001b[0m 51      \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 6.49e+03\u001b[0m | \u001b[0m 8.383   \u001b[0m |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 52      \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 6.322e+0\u001b[0m | \u001b[0m 499.7   \u001b[0m |\n",
      "| \u001b[0m 53      \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 6.344e+0\u001b[0m | \u001b[0m 498.9   \u001b[0m |\n",
      "| \u001b[0m 54      \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 6.377e+0\u001b[0m | \u001b[0m 6.299   \u001b[0m |\n",
      "| \u001b[0m 55      \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 6.559e+0\u001b[0m | \u001b[0m 498.7   \u001b[0m |\n",
      "| \u001b[0m 56      \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 6.652e+0\u001b[0m | \u001b[0m 3.838   \u001b[0m |\n",
      "| \u001b[0m 57      \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 6.8e+03 \u001b[0m | \u001b[0m 497.9   \u001b[0m |\n",
      "| \u001b[0m 58      \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 6.224e+0\u001b[0m | \u001b[0m 11.42   \u001b[0m |\n",
      "| \u001b[0m 59      \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 6.303e+0\u001b[0m | \u001b[0m 5.146   \u001b[0m |\n",
      "| \u001b[0m 60      \u001b[0m | \u001b[0m 0.3541  \u001b[0m | \u001b[0m 6.731e+0\u001b[0m | \u001b[0m 496.3   \u001b[0m |\n",
      "=================================================\n",
      "alpha is : 5510.212849815728\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ridge_logistic(X_train,y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
